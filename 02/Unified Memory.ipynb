{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><div align=\"center\">Managing Accelerated Application Memory with CUDA C/C++ Unified Memory</div></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CUDA](./images/CUDA_Logo.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [*CUDA Best Practices Guide*](http://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#memory-optimizations), a highly recommended followup to this and other CUDA fundamentals labs, recommends a design cycle called **APOD**: **A**ssess, **P**arallelize, **O**ptimize, **D**eploy. In short, APOD prescribes an iterative design process, where developers can apply incremental improvements to their accelerated application's performance, and ship their code. As developers become more competent CUDA programmers, more advanced optimization techniques can be applied to their accelerated code bases.\n",
    "\n",
    "This lab will support such a style of iterative development. You will be using the Nsight Systems command line tool **nsys** to qualitatively measure your application's performance, and to identify opportunities for optimization, after which you will apply incremental improvements before learning new techniques and repeating the cycle. As a point of focus, many of the techniques you will be learning and applying in this lab will deal with the specifics of how CUDA's **Unified Memory** works. Understanding Unified Memory behavior is a fundamental skill for CUDA developers, and serves as a prerequisite to many more advanced memory management techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prerequisites\n",
    "\n",
    "To get the most out of this lab you should already be able to:\n",
    "\n",
    "- Write, compile, and run C/C++ programs that both call CPU functions and launch GPU kernels.\n",
    "- Control parallel thread hierarchy using execution configuration.\n",
    "- Refactor serial loops to execute their iterations in parallel on a GPU.\n",
    "- Allocate and free Unified Memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Objectives\n",
    "\n",
    "By the time you complete this lab, you will be able to:\n",
    "\n",
    "- Use the Nsight Systems command line tool (**nsys**) to profile accelerated application performance.\n",
    "- Leverage an understanding of **Streaming Multiprocessors** to optimize execution configurations.\n",
    "- Understand the behavior of **Unified Memory** with regard to page faulting and data migrations.\n",
    "- Use **asynchronous memory prefetching** to reduce page faults and data migrations for increased performance.\n",
    "- Employ an iterative development cycle to rapidly accelerate and deploy applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Iterative Optimizations with the NVIDIA Command Line Profiler\n",
    "\n",
    "The only way to be assured that attempts at optimizing accelerated code bases are actually successful is to profile the application for quantitative information about the application's performance. `nsys` is the Nsight Systems command line tool. It ships with the CUDA toolkit, and is a powerful tool for profiling accelerated applications.\n",
    "\n",
    "`nsys` is easy to use. Its most basic usage is to simply pass it the path to an executable compiled with `nvcc`. `nsys` will proceed to execute the application, after which it will print a summary output of the application's GPU activities, CUDA API calls, as well as information about **Unified Memory** activity, a topic which will be covered extensively later in this lab.\n",
    "\n",
    "When accelerating applications, or optimizing already-accelerated applications, take a scientific and iterative approach. Profile your application after making changes, take note, and record the implications of any refactoring on performance. Make these observations early and often: frequently, enough performance boost can be gained with little effort such that you can ship your accelerated application. Additionally, frequent profiling will teach you how specific changes to your CUDA code bases impact its actual performance: knowledge that is hard to acquire when only profiling after many kinds of changes in your code bases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Profile an Application with nsys\n",
    "\n",
    "[01-vector-add.cu](01-vector-add/01-vector-add.cu) (<------ you can click on this and any of the source file links in this lab to open them for editing) is a naively accelerated vector addition program. Use the two code execution cells below (`CTRL` + `ENTER`). The first code execution cell will compile (and run) the vector addition program. The second code execution cell will profile the executable that was just compiled using `nsys profile`.\n",
    "\n",
    "`nsys profile` will generate a report file which can be used in a variety of manners, including for use in visual profiling with Nsight Systems, which we will look at in more detail in the following section.\n",
    "\n",
    "Here we use the `--stats=true` flag to indicate we would like summary statistics printed. In this section this summary will be the focus of our attention. There is quite a lot of information printed:\n",
    "\n",
    "- Operating System Runtime Summary (`osrt_sum`)\n",
    "- **CUDA API Summary (`cuda_api_sum`)**\n",
    "- **CUDA Kernel Summary (`cuda_gpu_kern_sum`)**\n",
    "- **CUDA Memory Time Operation Summary (`cuda_gpu_mem_time_sum`)**\n",
    "- **CUDA Memory Size Operation Summary (`cuda_gpu_mem_size_sum`)**\n",
    "\n",
    "In this section you will primarily be using the 4 summaries in **bold** above. In the next section, you will be using the generated report files to give to the Nsight Systems GUI for visual profiling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After profiling the application, answer the following questions using information displayed in the `cuda_gpu_kern_sum` section of the profiling output:\n",
    "\n",
    "- What was the name of the only CUDA kernel called in this application?\n",
    "- How many times did this kernel run?\n",
    "- How long did it take this kernel to run? Record this time somewhere: you will be optimizing this application and will want to know how much faster you can make it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o single-thread-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n",
      "Generating '/tmp/nsys-report-22af.qdstrm'\n",
      "[1/8] [========================100%] report1.nsys-rep\n",
      "[2/8] [========================100%] report1.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report1.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     90.4       6153767880        318  19351471.3  10072975.0      2190  100146202   27642884.9  poll                  \n",
      "      8.7        590217351        282   2092969.3   2065866.0       180   20497754    1267548.1  sem_timedwait         \n",
      "      0.6         44109596        499     88396.0     13371.0       520    9305295     523542.9  ioctl                 \n",
      "      0.3         19093859         24    795577.5      6245.0      1070    7173451    2142793.0  mmap                  \n",
      "      0.0          1117980         27     41406.7      4160.0      3290     709558     134496.2  mmap64                \n",
      "      0.0           531358         44     12076.3     11365.5      4581      29531       4929.4  open64                \n",
      "      0.0           165308         29      5700.3      3780.0      1480      33052       6359.3  fopen                 \n",
      "      0.0           149135          4     37283.8     35321.0     25451      53042      13913.4  pthread_create        \n",
      "      0.0           128466         11     11678.7     11831.0      1220      19761       6361.6  write                 \n",
      "      0.0            85382         12      7115.2      4320.0      1800      20791       6157.9  munmap                \n",
      "      0.0            56893         26      2188.2        80.0        70      54703      10711.0  fgets                 \n",
      "      0.0            46033          6      7672.2      7636.0      3330      12781       3217.5  open                  \n",
      "      0.0            36521         52       702.3       550.0       220       5650        755.2  fcntl                 \n",
      "      0.0            28821         22      1310.0      1095.0       690       2700        569.3  fclose                \n",
      "      0.0            22741         14      1624.4      1345.0       700       4070       1054.8  read                  \n",
      "      0.0            15171          2      7585.5      7585.5      4420      10751       4476.7  socket                \n",
      "      0.0            13080          5      2616.0      1490.0        70       7120       2931.2  fread                 \n",
      "      0.0            11920          1     11920.0     11920.0     11920      11920          0.0  connect               \n",
      "      0.0             6600         64       103.1       115.0        40        420         72.7  pthread_mutex_trylock \n",
      "      0.0             6490          1      6490.0      6490.0      6490       6490          0.0  pipe2                 \n",
      "      0.0             2591          1      2591.0      2591.0      2591       2591          0.0  bind                  \n",
      "      0.0             1270          1      1270.0      1270.0      1270       1270          0.0  listen                \n",
      "      0.0              650          1       650.0       650.0       650        650          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls    Avg (ns)      Med (ns)     Min (ns)    Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  ------------  ------------  ----------  ----------  -----------  ---------------------\n",
      "     94.1       2470023155          1  2470023155.0  2470023155.0  2470023155  2470023155          0.0  cudaDeviceSynchronize\n",
      "      5.2        136548141          3    45516047.0       62883.0       21810   136463448   78762762.4  cudaMallocManaged    \n",
      "      0.7         19148731          3     6382910.3     6096259.0     5831849     7220623     737428.0  cudaFree             \n",
      "      0.0            44692          1       44692.0       44692.0       44692       44692          0.0  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances    Avg (ns)      Med (ns)     Min (ns)    Max (ns)   StdDev (ns)                       Name                     \n",
      " --------  ---------------  ---------  ------------  ------------  ----------  ----------  -----------  ----------------------------------------------\n",
      "    100.0       2470014023          1  2470014023.0  2470014023.0  2470014023  2470014023          0.0  addVectorsInto(float *, float *, float *, int)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "     75.5         34177570   2304   14834.0    4383.0      1983     80287      22496.4  [CUDA Unified Memory memcpy HtoD]\n",
      "     24.5         11067281    768   14410.5    3727.5      1279     80767      22787.9  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    402.653   2304     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy HtoD]\n",
      "    134.218    768     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report1.nsys-rep\n",
      "    /dli/task/report1.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./single-thread-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worth mentioning is that by default, `nsys profile` will not overwrite an existing report file. This is done to prevent accidental loss of work when profiling. If for any reason, you would rather overwrite an existing report file, say during rapid iterations, you can provide the `-f` flag to `nsys profile` to allow overwriting an existing report file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Optimize and Profile\n",
    "\n",
    "Take a minute or two to make a simple optimization to [01-vector-add.cu](01-vector-add/01-vector-add.cu) by updating its execution configuration so that it runs on many threads in a single thread block. Recompile and then profile with `nsys profile --stats=true` using the code execution cells below. Use the profiling output to check the runtime of the kernel. What was the speed up from this optimization? Be sure to record your results somewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o multi-thread-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n",
      "Generating '/tmp/nsys-report-d43e.qdstrm'\n",
      "[1/8] [========================100%] report2.nsys-rep\n",
      "[2/8] [========================100%] report2.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report2.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     87.4       1797139559        100  17971395.6  10073873.5      2640  100143264   26326279.3  poll                  \n",
      "      9.4        193724251         89   2176677.0   2069654.0       130   20605190    2526039.5  sem_timedwait         \n",
      "      1.9         39977613        497     80437.9     12690.0       390    8094958     465079.4  ioctl                 \n",
      "      1.1         23548559         24    981190.0      5615.0       841    8643945    2643784.2  mmap                  \n",
      "      0.0           892001         27     33037.1      3990.0      2910     566688     107350.9  mmap64                \n",
      "      0.0           476467         44     10828.8     10115.5      3510      35071       5492.8  open64                \n",
      "      0.0           174716          4     43679.0     42936.5     31741      57102      12532.9  pthread_create        \n",
      "      0.0           142334         11     12939.5     14090.0      1100      20821       5012.5  write                 \n",
      "      0.0           136544         29      4708.4      3340.0       930      22691       4584.8  fopen                 \n",
      "      0.0            67711         12      5642.6      3860.0      1470      20780       5442.3  munmap                \n",
      "      0.0            50102         26      1927.0        70.0        60      48252       9448.5  fgets                 \n",
      "      0.0            35501          6      5916.8      6850.0      2480       8071       2202.6  open                  \n",
      "      0.0            31301         52       601.9       480.0       220       4380        594.8  fcntl                 \n",
      "      0.0            25763         22      1171.0       920.0       491       2710        652.3  fclose                \n",
      "      0.0            19673         14      1405.2      1160.0       420       3820        959.0  read                  \n",
      "      0.0            15481          2      7740.5      7740.5      3900      11581       5431.3  socket                \n",
      "      0.0            10480          1     10480.0     10480.0     10480      10480          0.0  connect               \n",
      "      0.0             6821          1      6821.0      6821.0      6821       6821          0.0  pipe2                 \n",
      "      0.0             6561         64       102.5       115.0        40        440         77.6  pthread_mutex_trylock \n",
      "      0.0             6500          5      1300.0      1160.0       150       2690       1184.5  fread                 \n",
      "      0.0             1940          1      1940.0      1940.0      1940       1940          0.0  bind                  \n",
      "      0.0             1351          1      1351.0      1351.0      1351       1351          0.0  listen                \n",
      "      0.0              260          1       260.0       260.0       260        260          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  -----------  -----------  ---------  ---------  -----------  ---------------------\n",
      "     50.7        128821902          3   42940634.0      54982.0      25780  128741140   74305419.3  cudaMallocManaged    \n",
      "     40.0        101467845          1  101467845.0  101467845.0  101467845  101467845          0.0  cudaDeviceSynchronize\n",
      "      9.3         23615323          3    7871774.3    7752168.0    7186880    8676275     751866.8  cudaFree             \n",
      "      0.0            44981          1      44981.0      44981.0      44981      44981          0.0  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)                       Name                     \n",
      " --------  ---------------  ---------  -----------  -----------  ---------  ---------  -----------  ----------------------------------------------\n",
      "    100.0        101458020          1  101458020.0  101458020.0  101458020  101458020          0.0  addVectorsInto(float *, float *, float *, int)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "     75.3         33719473   2304   14635.2    4111.0      1822     80031      22490.6  [CUDA Unified Memory memcpy HtoD]\n",
      "     24.7         11067890    768   14411.3    3711.0      1343     80575      22788.6  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    402.653   2304     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy HtoD]\n",
      "    134.218    768     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report2.nsys-rep\n",
      "    /dli/task/report2.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./multi-thread-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Optimize Iteratively\n",
    "\n",
    "In this exercise you will go through several cycles of editing the execution configuration of [01-vector-add.cu](01-vector-add/01-vector-add.cu), profiling it, and recording the results to see the impact. Use the following guidelines while working:\n",
    "\n",
    "- Start by listing 3 to 5 different ways you will update the execution configuration, being sure to cover a range of different grid and block size combinations.\n",
    "- Edit the [01-vector-add.cu](01-vector-add/01-vector-add.cu) program in one of the ways you listed.\n",
    "- Compile and profile your updated code with the two code execution cells below.\n",
    "- Record the runtime of the kernel execution, as given in the profiling output.\n",
    "- Repeat the edit/profile/record cycle for each possible optimization you listed above\n",
    "\n",
    "Which of the execution configurations you attempted proved to be the fastest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o iteratively-optimized-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n",
      "Generating '/tmp/nsys-report-c316.qdstrm'\n",
      "[1/8] [========================100%] report10.nsys-rep\n",
      "[2/8] [========================100%] report10.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report10.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     87.9       1776870328         99  17948185.1  10076779.0      1860  100144768   26590311.8  poll                  \n",
      "      9.4        189971334         89   2134509.4   2071178.0       210   20489176    2313921.1  sem_timedwait         \n",
      "      1.6         32459642        497     65311.2     11820.0       370    9296822     436548.3  ioctl                 \n",
      "      0.9         19183963         24    799331.8      4835.0       870    7144281    2151485.8  mmap                  \n",
      "      0.1          1082593         27     40096.0      4660.0      2910     721905     136912.4  mmap64                \n",
      "      0.0           496392         44     11281.6     10460.0      4420      30901       4967.2  open64                \n",
      "      0.0           173517          4     43379.3     42741.5     34682      53352       8720.6  pthread_create        \n",
      "      0.0           153466         29      5291.9      3490.0      1350      29762       5838.0  fopen                 \n",
      "      0.0           133434         11     12130.4     13161.0      1250      21721       5145.8  write                 \n",
      "      0.0            59342         11      5394.7      4080.0      1380      15380       4942.9  munmap                \n",
      "      0.0            56123         26      2158.6        90.0        70      53983      10570.2  fgets                 \n",
      "      0.0            37531          6      6255.2      6155.5      3560       9870       2295.7  open                  \n",
      "      0.0            34221         52       658.1       495.0       160       6480        861.0  fcntl                 \n",
      "      0.0            28790         22      1308.6      1220.0       690       3770        657.3  fclose                \n",
      "      0.0            19954         14      1425.3      1215.5       510       3841        998.9  read                  \n",
      "      0.0            15031          2      7515.5      7515.5      4110      10921       4816.1  socket                \n",
      "      0.0            11451          1     11451.0     11451.0     11451      11451          0.0  connect               \n",
      "      0.0             8481          5      1696.2      1720.0        70       3230       1461.2  fread                 \n",
      "      0.0             6821          1      6821.0      6821.0      6821       6821          0.0  pipe2                 \n",
      "      0.0             6790         64       106.1       120.0        40        490         78.8  pthread_mutex_trylock \n",
      "      0.0             2490          1      2490.0      2490.0      2490       2490          0.0  bind                  \n",
      "      0.0             1490          1      1490.0      1490.0      1490       1490          0.0  listen                \n",
      "      0.0              260          1       260.0       260.0       260        260          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ---------------------\n",
      "     50.1        109370707          3  36456902.3     30082.0     14040  109326585   63106996.9  cudaMallocManaged    \n",
      "     41.1         89791375          1  89791375.0  89791375.0  89791375   89791375          0.0  cudaDeviceSynchronize\n",
      "      8.8         19196744          3   6398914.7   6169264.0   5855649    7171831     687487.0  cudaFree             \n",
      "      0.1           125126          1    125126.0    125126.0    125126     125126          0.0  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)                       Name                     \n",
      " --------  ---------------  ---------  ----------  ----------  --------  --------  -----------  ----------------------------------------------\n",
      "    100.0         89848518          1  89848518.0  89848518.0  89848518  89848518          0.0  addVectorsInto(float *, float *, float *, int)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "     72.7         29285258   8191    3575.3    2111.0      1823     77856       6946.9  [CUDA Unified Memory memcpy HtoD]\n",
      "     27.3         11018891    760   14498.5    3727.5      1375     80543      22876.9  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    216.678   8191     0.026     0.004     0.004     1.016        0.093  [CUDA Unified Memory memcpy HtoD]\n",
      "    133.693    760     0.176     0.033     0.004     1.044        0.302  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report10.nsys-rep\n",
      "    /dli/task/report10.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./iteratively-optimized-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Streaming Multiprocessors and Querying the Device\n",
    "\n",
    "This section explores how understanding a specific feature of the GPU hardware can promote optimization. After introducing **Streaming Multiprocessors**, you will attempt to further optimize the accelerated vector addition program you have been working on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following video presents upcoming material visually, at a high level. Click watch it before moving on to more detailed coverage of their topics in following sections.\n",
    "\n",
    "<script>console.log('hi');</script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video controls width=\"640\" height=\"360\">\n",
       "    <source src=\"https://d36m44n9vdbmda.cloudfront.net/assets/s-ac-04-v1/task2/NVPROF_UM_1.mp4\" type=\"video/mp4\">\n",
       "    Your browser does not support the video tag.\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "video_url = \"https://d36m44n9vdbmda.cloudfront.net/assets/s-ac-04-v1/task2/NVPROF_UM_1.mp4\"\n",
    "\n",
    "video_html = f\"\"\"\n",
    "<video controls width=\"640\" height=\"360\">\n",
    "    <source src=\"{video_url}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "</video>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(video_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Multiprocessors and Warps\n",
    "\n",
    "The GPUs that CUDA applications run on have processing units called **streaming multiprocessors**, or **SMs**. During kernel execution, blocks of threads are given to SMs to execute. In order to support the GPU's ability to perform as many parallel operations as possible, performance gains can often be had by *choosing a grid size that has a number of blocks that is a multiple of the number of SMs on a given GPU.*\n",
    "\n",
    "Additionally, SMs create, manage, schedule, and execute groupings of 32 threads from within a block called **warps**. A more [in depth coverage of SMs and warps](http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-implementation) is beyond the scope of this course, however, it is important to know that performance gains can also be had by *choosing a block size that has a number of threads that is a multiple of 32.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programmatically Querying GPU Device Properties\n",
    "\n",
    "In order to support portability, since the number of SMs on a GPU can differ depending on the specific GPU being used, the number of SMs should not be hard-coded into a code bases. Rather, this information should be acquired programatically.\n",
    "\n",
    "The following shows how, in CUDA C/C++, to obtain a C struct which contains many properties about the currently active GPU device, including its number of SMs:\n",
    "\n",
    "```cpp\n",
    "int deviceId;\n",
    "cudaGetDevice(&deviceId);                  // `deviceId` now points to the id of the currently active GPU.\n",
    "\n",
    "cudaDeviceProp props;\n",
    "cudaGetDeviceProperties(&props, deviceId); // `props` now has many useful properties about\n",
    "                                           // the active GPU device.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Query the Device\n",
    "\n",
    "Currently, [01-get-device-properties.cu](04-device-properties/01-get-device-properties.cu) contains many unassigned variables, and will print gibberish information intended to describe details about the currently active GPU.\n",
    "\n",
    "Build out [01-get-device-properties.cu](04-device-properties/01-get-device-properties.cu) to print the actual values for the desired device properties indicated in the source code. In order to support your work, and as an introduction to them, use the [CUDA Runtime Docs](http://docs.nvidia.com/cuda/cuda-runtime-api/structcudaDeviceProp.html) to help identify the relevant properties in the device props struct. Refer to [the solution](04-device-properties/solutions/01-get-device-properties-solution.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01m\u001b[0m\u001b[01m04-device-properties/01-get-device-properties.cu(30)\u001b[0m: \u001b[01;35mwarning\u001b[0m: variable \u001b[01m\"computeCapabilityMinor\"\u001b[0m is used before its value is set\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m04-device-properties/01-get-device-properties.cu(30)\u001b[0m: \u001b[01;35mwarning\u001b[0m: variable \u001b[01m\"computeCapabilityMinor\"\u001b[0m is used before its value is set\n",
      "\n",
      "Device ID: 0\n",
      "Number of SMs: 80\n",
      "Compute Capability Major: 8\n",
      "Compute Capability Minor: 0\n",
      "Warp Size: 32\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o get-device-properties 04-device-properties/01-get-device-properties.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Optimize Vector Add with Grids Sized to Number of SMs\n",
    "\n",
    "Utilize your ability to query the device for its number of SMs to refactor the `addVectorsInto` kernel you have been working on inside [01-vector-add.cu](01-vector-add/01-vector-add.cu) so that it launches with a grid containing a number of blocks that is a multiple of the number of SMs on the device.\n",
    "\n",
    "Depending on other specific details in the code you have written, this refactor may or may not improve, or significantly change, the performance of your kernel. Therefore, as always, be sure to use `nsys profile` so that you can quantitatively evaluate performance changes. Record the results with the rest of your findings thus far, based on the profiling output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o sm-optimized-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n",
      "Generating '/tmp/nsys-report-c106.qdstrm'\n",
      "[1/8] [========================100%] report12.nsys-rep\n",
      "[2/8] [========================100%] report12.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report12.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     87.6       1771162070         98  18073082.3  10069410.0     20621  100151458   26640676.7  poll                  \n",
      "      9.4        190797268         88   2168150.8   2065724.0       170   20581119    2504683.7  sem_timedwait         \n",
      "      1.8         36815800        497     74076.1     12210.0       380    9061497     503871.5  ioctl                 \n",
      "      1.0         20096955         24    837373.1      6045.5       870    7253880    2251131.4  mmap                  \n",
      "      0.1          1068705         27     39581.7      4360.0      3180     713490     135361.2  mmap64                \n",
      "      0.0           504683         44     11470.1     11045.5      5520      32922       4255.1  open64                \n",
      "      0.0           202640          4     50660.0     48437.5     38302      67463      14637.5  pthread_create        \n",
      "      0.0           174677         29      6023.3      4140.0      1360      33962       6817.2  fopen                 \n",
      "      0.0           149095         11     13554.1     14040.0     10531      17911       2412.9  write                 \n",
      "      0.0            83433         12      6952.8      4110.5      1790      19230       6104.8  munmap                \n",
      "      0.0            56702         26      2180.8        80.0        70      54592      10689.8  fgets                 \n",
      "      0.0            44082          6      7347.0      8360.5      3680       9821       2516.3  open                  \n",
      "      0.0            34860         52       670.4       500.0       180       5950        788.5  fcntl                 \n",
      "      0.0            30130         22      1369.5      1240.0       690       4430        786.1  fclose                \n",
      "      0.0            23040         14      1645.7      1185.0       760       4230       1050.0  read                  \n",
      "      0.0            15971          2      7985.5      7985.5      3981      11990       5663.2  socket                \n",
      "      0.0            12161          1     12161.0     12161.0     12161      12161          0.0  connect               \n",
      "      0.0             7291          1      7291.0      7291.0      7291       7291          0.0  pipe2                 \n",
      "      0.0             6270          5      1254.0       980.0        70       2600       1172.6  fread                 \n",
      "      0.0             5350         64        83.6        50.0        40        200         45.8  pthread_mutex_trylock \n",
      "      0.0             2820          1      2820.0      2820.0      2820       2820          0.0  bind                  \n",
      "      0.0             1200          1      1200.0      1200.0      1200       1200          0.0  listen                \n",
      "      0.0              270          1       270.0       270.0       270        270          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ---------------------\n",
      "     52.5        113813332          3  37937777.3     55272.0     28272  113729788   65637808.0  cudaMallocManaged    \n",
      "     38.2         82680872          1  82680872.0  82680872.0  82680872   82680872          0.0  cudaDeviceSynchronize\n",
      "      9.3         20161922          3   6720640.7   6681286.0   6177874    7302762     563475.7  cudaFree             \n",
      "      0.0            44432          1     44432.0     44432.0     44432      44432          0.0  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)                       Name                     \n",
      " --------  ---------------  ---------  ----------  ----------  --------  --------  -----------  ----------------------------------------------\n",
      "    100.0         82671391          1  82671391.0  82671391.0  82671391  82671391          0.0  addVectorsInto(float *, float *, float *, int)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "     81.4         48487247  11497    4217.4    2175.0      1823     80063       9294.3  [CUDA Unified Memory memcpy HtoD]\n",
      "     18.6         11059532    768   14400.4    3711.5      1247     80575      22786.3  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    402.653  11497     0.035     0.008     0.004     1.044        0.124  [CUDA Unified Memory memcpy HtoD]\n",
      "    134.218    768     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report12.nsys-rep\n",
      "    /dli/task/report12.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./sm-optimized-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Unified Memory Details\n",
    "\n",
    "You have been allocating memory intended for use either by host or device code with `cudaMallocManaged` and up until now have enjoyed the benefits of this method - automatic memory migration, ease of programming - without diving into the details of how the **Unified Memory** (**UM**) allocated by `cudaMallocManaged` actual works.\n",
    "\n",
    "`nsys profile` provides details about UM management in accelerated applications, and using this information, in conjunction with a more-detailed understanding of how UM works, provides additional opportunities to optimize accelerated applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following video presents upcoming material visually, at a high level. Click watch it before moving on to more detailed coverage of their topics in following sections.\n",
    "\n",
    "<script>console.log('hi');</script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video controls width=\"640\" height=\"360\">\n",
       "    <source src=\"https://d36m44n9vdbmda.cloudfront.net/assets/s-ac-04-v1/task2/NVPROF_UM_2.mp4\" type=\"video/mp4\">\n",
       "    Your browser does not support the video tag.\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "video_url = \"https://d36m44n9vdbmda.cloudfront.net/assets/s-ac-04-v1/task2/NVPROF_UM_2.mp4\"\n",
    "\n",
    "video_html = f\"\"\"\n",
    "<video controls width=\"640\" height=\"360\">\n",
    "    <source src=\"{video_url}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "</video>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(video_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unified Memory Migration\n",
    "\n",
    "When UM is allocated, the memory is not resident yet on either the host or the device. When either the host or device attempts to access the memory, a [page fault](https://en.wikipedia.org/wiki/Page_fault) will occur, at which point the host or device will migrate the needed data in batches. Similarly, at any point when the CPU, or any GPU in the accelerated system, attempts to access memory not yet resident on it, page faults will occur and trigger its migration.\n",
    "\n",
    "The ability to page fault and migrate memory on demand is tremendously helpful for ease of development in your accelerated applications. Additionally, when working with data that exhibits sparse access patterns, for example when it is impossible to know which data will be required to be worked on until the application actually runs, and for scenarios when data might be accessed by multiple GPU devices in an accelerated system with multiple GPUs, on-demand memory migration is remarkably beneficial.\n",
    "\n",
    "There are times - for example when data needs are known prior to runtime, and large contiguous blocks of memory are required - when the overhead of page faulting and migrating data on demand incurs an overhead cost that would be better avoided.\n",
    "\n",
    "Much of the remainder of this lab will be dedicated to understanding on-demand migration, and how to identify it in the profiler's output. With this knowledge you will be able to reduce the overhead of it in scenarios when it would be beneficial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Explore UM Migration and Page Faulting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nsys profile` provides output describing UM behavior for the profiled application. In this exercise, you will make several modifications to a simple application, and make use of `nsys profile` after each change, to explore how UM data migration behaves.\n",
    "\n",
    "[01-page-faults.cu](06-unified-memory-page-faults/01-page-faults.cu) contains a `hostFunction` and a `gpuKernel`, both which could be used to initialize the elements of a `2<<24` element vector with the number `1`. Currently neither the host function nor GPU kernel are being used.\n",
    "\n",
    "For each of the 4 questions below, given what you have just learned about UM behavior, first hypothesize about what kind of page faulting should happen, then, edit [01-page-faults.cu](06-unified-memory-page-faults/01-page-faults.cu) to create a scenario, by using one or both of the 2 provided functions in the code bases, that will allow you to test your hypothesis.\n",
    "\n",
    "In order to test your hypotheses, compile and profile your code using the code execution cells below. Be sure to record your hypotheses, as well as the results, obtained from `nsys profile --stats=true` output. In the output of `nsys profile --stats=true` you should be looking for the following:\n",
    "\n",
    "- Is there a _CUDA Memory Operation Statistics_ section in the output?\n",
    "- If so, does it indicate host to device (HtoD) or device to host (DtoH) migrations?\n",
    "- When there are migrations, what does the output say about how many _Operations_ there were? If you see many small memory migration operations, this is a sign that on-demand page faulting is occurring, with small memory migrations occurring each time there is a page fault in the requested location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the scenarios for you to explore, along with solutions for them if you get stuck:\n",
    "\n",
    "- Is there evidence of memory migration and/or page faulting when unified memory is accessed only by the CPU? ([solution](06-unified-memory-page-faults/solutions/01-page-faults-solution-cpu-only.cu))\n",
    "- Is there evidence of memory migration and/or page faulting when unified memory is accessed only by the GPU? ([solution](06-unified-memory-page-faults/solutions/02-page-faults-solution-gpu-only.cu))\n",
    "- Is there evidence of memory migration and/or page faulting when unified memory is accessed first by the CPU then the GPU? ([solution](06-unified-memory-page-faults/solutions/03-page-faults-solution-cpu-then-gpu.cu))\n",
    "- Is there evidence of memory migration and/or page faulting when unified memory is accessed first by the GPU then the CPU? ([solution](06-unified-memory-page-faults/solutions/04-page-faults-solution-gpu-then-cpu.cu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvcc -o page-faults 06-unified-memory-page-faults/01-page-faults.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating '/tmp/nsys-report-30f3.qdstrm'\n",
      "[1/8] [========================100%] report22.nsys-rep\n",
      "[2/8] [========================100%] report22.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report22.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     82.3        532754184         37  14398761.7  10070509.0      2330  100134711   22316975.8  poll                  \n",
      "     11.3         72983886         33   2211632.9   2066606.0       150   20423150    3766942.9  sem_timedwait         \n",
      "      4.9         31618672        483     65463.1     11481.0       490    8712845     417045.7  ioctl                 \n",
      "      1.2          8000592         18    444477.3      7135.5       850    7861305    1851013.7  mmap                  \n",
      "      0.1           950595         27     35207.2      4060.0      3010     602408     114093.5  mmap64                \n",
      "      0.1           520936         44     11839.5     11221.0      3530      33531       5213.4  open64                \n",
      "      0.0           223730          4     55932.5     53587.5     37132      79423      17539.7  pthread_create        \n",
      "      0.0           185558         29      6398.6      4131.0      1540      35772       7299.9  fopen                 \n",
      "      0.0           145646         11     13240.5     13840.0      1210      18581       4389.3  write                 \n",
      "      0.0            76283          7     10897.6      4130.0      1980      43082      14976.6  munmap                \n",
      "      0.0            50813         26      1954.3        70.0        60      49083       9612.4  fgets                 \n",
      "      0.0            43733          6      7288.8      7950.5      2431      12321       3490.9  open                  \n",
      "      0.0            35513         52       682.9       460.5       160       7370       1008.4  fcntl                 \n",
      "      0.0            29281         22      1331.0      1110.0       510       4090        764.2  fclose                \n",
      "      0.0            22172         14      1583.7      1230.5       810       4220        975.7  read                  \n",
      "      0.0            17920          2      8960.0      8960.0      4740      13180       5968.0  socket                \n",
      "      0.0            14440          1     14440.0     14440.0     14440      14440          0.0  connect               \n",
      "      0.0             7930          5      1586.0      1230.0        80       3190       1481.7  fread                 \n",
      "      0.0             6700          1      6700.0      6700.0      6700       6700          0.0  pipe2                 \n",
      "      0.0             5850         64        91.4        50.0        40        470         68.5  pthread_mutex_trylock \n",
      "      0.0             2500          1      2500.0      2500.0      2500       2500          0.0  bind                  \n",
      "      0.0             1321          1      1321.0      1321.0      1321       1321          0.0  listen                \n",
      "      0.0              160          1       160.0       160.0       160        160          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  -----------  -----------  ---------  ---------  -----------  ---------------------\n",
      "     93.2        108936731          1  108936731.0  108936731.0  108936731  108936731          0.0  cudaMallocManaged    \n",
      "      6.8          7955350          1    7955350.0    7955350.0    7955350    7955350          0.0  cudaFree             \n",
      "      0.0            26002          1      26002.0      26002.0      26002      26002          0.0  cudaLaunchKernel     \n",
      "      0.0            20231          1      20231.0      20231.0      20231      20231          0.0  cudaDeviceSynchronize\n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)            Name          \n",
      " --------  ---------------  ---------  ----------  ----------  --------  --------  -----------  ------------------------\n",
      "    100.0         12126329          1  12126329.0  12126329.0  12126329  12126329          0.0  deviceKernel(int *, int)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "     91.2         11086008    807   13737.3    1631.0      1279     87008      22260.2  [CUDA Unified Memory memcpy DtoH]\n",
      "      8.8          1075146    336    3199.8    2111.0      2047     77184       5485.8  [CUDA Unified Memory memcpy HtoD]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    133.153    807     0.165     0.004     0.004     1.044        0.293  [CUDA Unified Memory memcpy DtoH]\n",
      "      6.324    336     0.019     0.004     0.004     1.008        0.073  [CUDA Unified Memory memcpy HtoD]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report22.nsys-rep\n",
      "    /dli/task/report22.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./page-faults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Revisit UM Behavior for Vector Add Program\n",
    "\n",
    "Returning to the [01-vector-add.cu](01-vector-add/01-vector-add.cu) program you have been working on throughout this lab, review the code bases in its current state, and hypothesize about what kinds of memory migrations and/or page faults you expect to occur. Look at the profiling output for your last refactor (either by scrolling up to find the output or by executing the code execution cell just below), observing the _CUDA Memory Operation Statistics_ section of the profiler output. Can you explain the kinds of migrations and the number of their operations based on the contents of the code base?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n",
      "Generating '/tmp/nsys-report-a07f.qdstrm'\n",
      "[1/8] [========================100%] report21.nsys-rep\n",
      "[2/8] [========================100%] report21.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report21.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     87.6       1775810486         98  18120515.2  10072152.0      2251  100135422   26521067.9  poll                  \n",
      "      9.2        187357991         88   2129068.1   2065722.0       190   20499920    2304915.6  sem_timedwait         \n",
      "      2.1         41842986        497     84191.1     11870.0       380    8286258     487878.6  ioctl                 \n",
      "      1.0         20695390         24    862307.9      7020.5       710    8040297    2332917.2  mmap                  \n",
      "      0.0           921432         27     34127.1      4040.0      2790     546455     103625.7  mmap64                \n",
      "      0.0           521155         44     11844.4     10506.0      3910      35822       6348.3  open64                \n",
      "      0.0           199087         29      6865.1      4020.0      1440      50612       9883.9  fopen                 \n",
      "      0.0           183589          4     45897.3     46072.0     35582      55863       9468.8  pthread_create        \n",
      "      0.0           135336         11     12303.3     13440.0      1040      21591       5191.8  write                 \n",
      "      0.0            52942         11      4812.9      3150.0      1520      18971       5013.8  munmap                \n",
      "      0.0            49643         26      1909.3        70.0        50      47873       9374.8  fgets                 \n",
      "      0.0            41373          6      6895.5      8105.5      2760       9981       2837.8  open                  \n",
      "      0.0            36632         52       704.5       485.0       220       6720        913.5  fcntl                 \n",
      "      0.0            29812         22      1355.1      1050.5       530       4470        851.9  fclose                \n",
      "      0.0            20150         14      1439.3      1250.0       520       4290       1062.6  read                  \n",
      "      0.0            13180          2      6590.0      6590.0      3930       9250       3761.8  socket                \n",
      "      0.0            10150          5      2030.0      1470.0       150       4940       1989.5  fread                 \n",
      "      0.0             8570          1      8570.0      8570.0      8570       8570          0.0  connect               \n",
      "      0.0             7751          1      7751.0      7751.0      7751       7751          0.0  pipe2                 \n",
      "      0.0             6380         64        99.7       125.0        40        270         52.4  pthread_mutex_trylock \n",
      "      0.0             2021          1      2021.0      2021.0      2021       2021          0.0  bind                  \n",
      "      0.0             1340          1      1340.0      1340.0      1340       1340          0.0  listen                \n",
      "      0.0              350          1       350.0       350.0       350        350          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ---------------------\n",
      "     54.2        134750404          3  44916801.3     40192.0     14190  134696022   77751086.9  cudaMallocManaged    \n",
      "     37.5         93274901          1  93274901.0  93274901.0  93274901   93274901          0.0  cudaDeviceSynchronize\n",
      "      8.3         20724941          3   6908313.7   6501755.0   6147258    8075928    1026600.9  cudaFree             \n",
      "      0.0            46952          1     46952.0     46952.0     46952      46952          0.0  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)                       Name                     \n",
      " --------  ---------------  ---------  ----------  ----------  --------  --------  -----------  ----------------------------------------------\n",
      "    100.0         93264564          1  93264564.0  93264564.0  93264564  93264564          0.0  addVectorsInto(float *, float *, float *, int)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "     80.9         46915085  10546    4448.6    2175.0      1823     79071       9680.4  [CUDA Unified Memory memcpy HtoD]\n",
      "     19.1         11069209    768   14413.0    3743.5      1279     80800      22796.4  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    402.653  10546     0.038     0.008     0.004     1.032        0.129  [CUDA Unified Memory memcpy HtoD]\n",
      "    134.218    768     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report21.nsys-rep\n",
      "    /dli/task/report21.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./sm-optimized-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Initialize Vector in Kernel\n",
    "\n",
    "When `nsys profile` gives the amount of time that a kernel takes to execute, the host-to-device page faults and data migrations that occur during this kernel's execution are included in the displayed execution time.\n",
    "\n",
    "With this in mind, refactor the `initWith` host function in your [01-vector-add.cu](01-vector-add/01-vector-add.cu) program to instead be a CUDA kernel, initializing the allocated vector in parallel on the GPU. After successfully compiling and running the refactored application, but before profiling it, hypothesize about the following:\n",
    "\n",
    "- How do you expect the refactor to affect UM memory migration behavior?\n",
    "- How do you expect the refactor to affect the reported run time of `addVectorsInto`?\n",
    "\n",
    "Once again, record the results. Refer to [the solution](07-init-in-kernel/solutions/01-vector-add-init-in-kernel-solution.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o initialize-in-kernel 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n",
      "Generating '/tmp/nsys-report-72be.qdstrm'\n",
      "[1/8] [========================100%] report23.nsys-rep\n",
      "[2/8] [========================100%] report23.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report23.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     80.3        501831549         34  14759751.4  10070010.0      1900  100128953   23055483.4  poll                  \n",
      "     10.8         67324123         30   2244137.4   2063564.0       220   20459965    4008719.8  sem_timedwait         \n",
      "      5.5         34472568        497     69361.3     12401.0       510    9551026     449627.5  ioctl                 \n",
      "      3.0         18873203         24    786383.5      5505.5       920    9935514    2307118.5  mmap                  \n",
      "      0.2          1122660         27     41580.0      4620.0      3160     714623     135511.7  mmap64                \n",
      "      0.1           561336         44     12757.6     11630.5      4380      53412       7435.5  open64                \n",
      "      0.0           198960          4     49740.0     49057.5     35862      64983      14539.4  pthread_create        \n",
      "      0.0           174148         29      6005.1      3660.0      1700      33271       6503.1  fopen                 \n",
      "      0.0           144378         11     13125.3     13241.0      1310      18181       4762.3  write                 \n",
      "      0.0            57313         26      2204.3        80.0        70      55133      10795.4  fgets                 \n",
      "      0.0            45712          6      7618.7      8975.5      3360       9990       2871.3  open                  \n",
      "      0.0            40572         11      3688.4      3650.0      1760       6990       1748.3  munmap                \n",
      "      0.0            38312         52       736.8       530.0       150       6680        925.0  fcntl                 \n",
      "      0.0            30520         22      1387.3      1270.0       720       4240        745.2  fclose                \n",
      "      0.0            22281         14      1591.5      1260.0       430       4130       1063.1  read                  \n",
      "      0.0            20890          5      4178.0      3310.0        90       9560       3746.9  fread                 \n",
      "      0.0            16881          2      8440.5      8440.5      3660      13221       6760.6  socket                \n",
      "      0.0            12061          1     12061.0     12061.0     12061      12061          0.0  connect               \n",
      "      0.0             6400          1      6400.0      6400.0      6400       6400          0.0  pipe2                 \n",
      "      0.0             6230         64        97.3        80.0        40        280         60.6  pthread_mutex_trylock \n",
      "      0.0             2370          1      2370.0      2370.0      2370       2370          0.0  bind                  \n",
      "      0.0             1180          1      1180.0      1180.0      1180       1180          0.0  listen                \n",
      "      0.0              340          1       340.0       340.0       340        340          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ---------------------\n",
      "     66.2        119225956          3  39741985.3     30102.0     19751  119176103   68791964.0  cudaMallocManaged    \n",
      "     23.2         41797999          1  41797999.0  41797999.0  41797999   41797999          0.0  cudaDeviceSynchronize\n",
      "     10.5         18916894          3   6305631.3   4513576.0   4411801    9991517    3192476.2  cudaFree             \n",
      "      0.0            48112          4     12028.0      6720.5      5720      28951      11297.3  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)                       Name                     \n",
      " --------  ---------------  ---------  ----------  ----------  --------  --------  -----------  ----------------------------------------------\n",
      "     98.0         40962161          3  13654053.7  13509894.0  13477063  13975204     278608.4  initWith(float, float *, int)                 \n",
      "      2.0           848317          1    848317.0    848317.0    848317    848317          0.0  addVectorsInto(float *, float *, float *, int)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    100.0         11089770    768   14439.8    3759.5      1439     81184      22786.8  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    134.218    768     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report23.nsys-rep\n",
      "    /dli/task/report23.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./initialize-in-kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Asynchronous Memory Prefetching\n",
    "\n",
    "A powerful technique to reduce the overhead of page faulting and on-demand memory migrations, both in host-to-device and device-to-host memory transfers, is called **asynchronous memory prefetching**. Using this technique allows programmers to asynchronously migrate unified memory (UM) to any CPU or GPU device in the system, in the background, prior to its use by application code. By doing this, GPU kernels and CPU function performance can be increased on account of reduced page fault and on-demand data migration overhead.\n",
    "\n",
    "Prefetching also tends to migrate data in larger chunks, and therefore fewer trips, than on-demand migration. This makes it an excellent fit when data access needs are known before runtime, and when data access patterns are not sparse.\n",
    "\n",
    "CUDA Makes asynchronously prefetching managed memory to either a GPU device or the CPU easy with its `cudaMemPrefetchAsync` function. Here is an example of using it to both prefetch data to the currently active GPU device, and then, to the CPU:\n",
    "\n",
    "```cpp\n",
    "int deviceId;\n",
    "cudaGetDevice(&deviceId);                                         // The ID of the currently active GPU device.\n",
    "\n",
    "cudaMemPrefetchAsync(pointerToSomeUMData, size, deviceId);        // Prefetch to GPU device.\n",
    "cudaMemPrefetchAsync(pointerToSomeUMData, size, cudaCpuDeviceId); // Prefetch to host. `cudaCpuDeviceId` is a\n",
    "                                                                  // built-in CUDA variable.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Prefetch Memory\n",
    "\n",
    "At this point in the lab, your [01-vector-add.cu](01-vector-add/01-vector-add.cu) program should not only be launching a CUDA kernel to add 2 vectors into a third solution vector, all which are allocated with `cudaMallocManaged`, but should also be initializing each of the 3 vectors in parallel in a CUDA kernel. If for some reason, your application does not do any of the above, please refer to the following [reference application](07-init-in-kernel/solutions/01-vector-add-init-in-kernel-solution.cu), and update your own code bases to reflect its current functionality.\n",
    "\n",
    "Conduct 3 experiments using `cudaMemPrefetchAsync` inside of your [01-vector-add.cu](01-vector-add/01-vector-add.cu) application to understand its impact on page-faulting and memory migration.\n",
    "\n",
    "- What happens when you prefetch one of the initialized vectors to the device?\n",
    "- What happens when you prefetch two of the initialized vectors to the device?\n",
    "- What happens when you prefetch all three of the initialized vectors to the device?\n",
    "\n",
    "Hypothesize about UM behavior, page faulting specifically, as well as the impact on the reported run time of the initialization kernel, before each experiment, and then verify by running `nsys profile`. Refer to [the solution](08-prefetch/solutions/01-vector-add-prefetch-solution.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o prefetch-to-gpu 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n",
      "Generating '/tmp/nsys-report-a20b.qdstrm'\n",
      "[1/8] [========================100%] report27.nsys-rep\n",
      "[2/8] [========================100%] report27.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report27.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     75.8        371276738         30  12375891.3  10069275.0      2220  100074085   18946624.0  poll                  \n",
      "     13.6         66806522         27   2474315.6   2064214.0       160   20493400    4791930.8  sem_timedwait         \n",
      "      6.5         32048486        500     64097.0     10885.0       380    8344750     393585.2  ioctl                 \n",
      "      3.6         17597089         24    733212.0      4275.5       850    9148624    2141418.5  mmap                  \n",
      "      0.2           895822         27     33178.6      4721.0      2890     559171     105834.7  mmap64                \n",
      "      0.1           512132         44     11639.4     10720.5      2820      32692       5707.2  open64                \n",
      "      0.0           172991         29      5965.2      3600.0      1000      45082       8676.4  fopen                 \n",
      "      0.0           168259          4     42064.8     36652.0     33112      61843      13393.6  pthread_create        \n",
      "      0.0           144019         11     13092.6     14361.0      1140      18301       4274.1  write                 \n",
      "      0.0            49262         26      1894.7        70.0        50      47572       9316.4  fgets                 \n",
      "      0.0            39152          6      6525.3      7505.0      2580       8921       2757.5  open                  \n",
      "      0.0            38231         11      3475.5      3240.0      1140       6030       1378.1  munmap                \n",
      "      0.0            34870         52       670.6       470.0       160       5940        823.8  fcntl                 \n",
      "      0.0            27303         22      1241.0      1000.0       520       4121        775.2  fclose                \n",
      "      0.0            21600         14      1542.9      1240.0       840       3830        889.0  read                  \n",
      "      0.0            13960          2      6980.0      6980.0      4230       9730       3889.1  socket                \n",
      "      0.0             9760          1      9760.0      9760.0      9760       9760          0.0  connect               \n",
      "      0.0             9182          5      1836.4      1770.0        90       3071       1256.4  fread                 \n",
      "      0.0             6350         64        99.2        60.0        40        440         66.8  pthread_mutex_trylock \n",
      "      0.0             5981          1      5981.0      5981.0      5981       5981          0.0  pipe2                 \n",
      "      0.0             2140          1      2140.0      2140.0      2140       2140          0.0  bind                  \n",
      "      0.0             1170          1      1170.0      1170.0      1170       1170          0.0  listen                \n",
      "      0.0              340          1       340.0       340.0       340        340          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)   Med (ns)   Min (ns)  Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  ----------  ---------  --------  ---------  -----------  ---------------------\n",
      "     83.6        107052249          3  35684083.0    27772.0     14460  107010017   61770071.2  cudaMallocManaged    \n",
      "     13.8         17626142          3   5875380.7  4237294.0   4204881    9183967    2865365.6  cudaFree             \n",
      "      1.3          1707014          1   1707014.0  1707014.0   1707014    1707014          0.0  cudaDeviceSynchronize\n",
      "      1.2          1580697          3    526899.0   531449.0    502598     546650      22375.7  cudaMemPrefetchAsync \n",
      "      0.0            48293          4     12073.3     7671.0      6510      26441       9624.1  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)                       Name                     \n",
      " --------  ---------------  ---------  --------  --------  --------  --------  -----------  ----------------------------------------------\n",
      "     50.4           869413          1  869413.0  869413.0    869413    869413          0.0  addVectorsInto(float *, float *, float *, int)\n",
      "     49.6           856933          3  285644.3  286626.0    283073    287234       2247.5  initWith(float, float *, int)                 \n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    100.0         11080821    768   14428.2    3743.5      1407     80673      22785.1  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    134.218    768     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report27.nsys-rep\n",
      "    /dli/task/report27.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./prefetch-to-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Prefetch Memory Back to the CPU\n",
    "\n",
    "Add additional prefetching back to the CPU for the function that verifies the correctness of the `addVectorInto` kernel. Again, hypothesize about the impact on UM before profiling in `nsys` to confirm. Refer to [the solution](08-prefetch/solutions/02-vector-add-prefetch-solution-cpu-also.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o prefetch-to-cpu 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n",
      "Generating '/tmp/nsys-report-ca89.qdstrm'\n",
      "[1/8] [========================100%] report29.nsys-rep\n",
      "[2/8] [========================100%] report29.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report29.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     69.9        492156400         37  13301524.3  10069802.0      2260  100126634   17917926.7  poll                  \n",
      "     16.8        118004272        503    234600.9     10570.0       390   29292328    2249709.3  ioctl                 \n",
      "      9.7         68240468         29   2353119.6   2063089.0       280   20429471    4067889.5  sem_timedwait         \n",
      "      3.1         21525349         24    896889.5      4575.0      1080    7620263    2411626.3  mmap                  \n",
      "      0.3          1796365          2    898182.5    898182.5     76964    1719401    1161378.3  sem_wait              \n",
      "      0.1           889499         27     32944.4      4501.0      2930     546919     103597.7  mmap64                \n",
      "      0.1           462020         44     10500.5      9260.5      3390      31061       5158.2  open64                \n",
      "      0.0           228282          5     45656.4     47042.0     28711      66444      15523.0  pthread_create        \n",
      "      0.0           148336         29      5115.0      3590.0      1430      21911       4652.1  fopen                 \n",
      "      0.0           136349         12     11362.4     12345.5       860      17281       4392.5  write                 \n",
      "      0.0            64784         11      5889.5      3350.0      1370      21151       6539.7  munmap                \n",
      "      0.0            51213         26      1969.7        70.0        60      49443       9682.7  fgets                 \n",
      "      0.0            35391          6      5898.5      6715.5      2990       8380       2124.3  open                  \n",
      "      0.0            32850         52       631.7       470.0       150       6110        834.2  fcntl                 \n",
      "      0.0            25702         22      1168.3       960.0       490       3540        691.2  fclose                \n",
      "      0.0            20670         15      1378.0      1050.0       380       3510       1008.4  read                  \n",
      "      0.0            16541          2      8270.5      8270.5      4330      12211       5572.7  socket                \n",
      "      0.0            12261          1     12261.0     12261.0     12261      12261          0.0  connect               \n",
      "      0.0             9851          5      1970.2      1640.0        90       3550       1408.8  fread                 \n",
      "      0.0             6450          1      6450.0      6450.0      6450       6450          0.0  pipe2                 \n",
      "      0.0             6080         64        95.0        50.0        40        470         80.2  pthread_mutex_trylock \n",
      "      0.0             2360          1      2360.0      2360.0      2360       2360          0.0  bind                  \n",
      "      0.0             1280          1      1280.0      1280.0      1280       1280          0.0  listen                \n",
      "      0.0              270          1       270.0       270.0       270        270          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ---------------------\n",
      "     49.0        107350586          3  35783528.7     29812.0     13850  107306924   61941077.8  cudaMallocManaged    \n",
      "     40.4         88549952          1  88549952.0  88549952.0  88549952   88549952          0.0  cudaDeviceSynchronize\n",
      "      9.8         21587421          3   7195807.0   7118886.0   6797340    7671195     441976.5  cudaFree             \n",
      "      0.8          1742961          6    290493.5    328507.0      4830     561549     263730.9  cudaMemPrefetchAsync \n",
      "      0.0            37552          4      9388.0      5120.5      4220      23091       9153.6  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)                       Name                     \n",
      " --------  ---------------  ---------  --------  --------  --------  --------  -----------  ----------------------------------------------\n",
      "     50.7           882531          1  882531.0  882531.0    882531    882531          0.0  addVectorsInto(float *, float *, float *, int)\n",
      "     49.3           858947          3  286315.7  287521.0    283329    288097       2602.5  initWith(float, float *, int)                 \n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    100.0         30756642    192  160190.8  160176.5    160000    160704        107.0  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    402.653    192     2.097     2.097     2.097     2.097        0.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report29.nsys-rep\n",
      "    /dli/task/report29.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./prefetch-to-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this series of refactors to use asynchronous prefetching, you should see that there are fewer, but larger, memory transfers, and, that the kernel execution time is significantly decreased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "At this point in the lab, you are able to:\n",
    "\n",
    "- Use the Nsight Systems command line tool (**nsys**) to profile accelerated application performance.\n",
    "- Leverage an understanding of **Streaming Multiprocessors** to optimize execution configurations.\n",
    "- Understand the behavior of **Unified Memory** with regard to page faulting and data migrations.\n",
    "- Use **asynchronous memory prefetching** to reduce page faults and data migrations for increased performance.\n",
    "- Employ an iterative development cycle to rapidly accelerate and deploy applications.\n",
    "\n",
    "In order to consolidate your learning, and reinforce your ability to iteratively accelerate, optimize, and deploy applications, please proceed to this lab's final exercise. After completing it, for those of you with time and interest, please proceed to the *Advanced Content* section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Final Exercise: Iteratively Optimize an Accelerated SAXPY Application\n",
    "\n",
    "A basic accelerated SAXPY (Single Precision a\\*x+b) application has been provided for you [here](09-saxpy/01-saxpy.cu). It currently works and you can compile, run, and then profile it with `nsys profile` below.\n",
    "\n",
    "Record the runtime of the `saxpy` kernel without making any modifications and then work *iteratively* to optimize the application, using `nsys profile` after each iteration to notice the effects of the code changes on kernel performance and UM behavior.\n",
    "\n",
    "Utilize the techniques from this lab. To support your learning, utilize [effortful retrieval](http://sites.gsu.edu/scholarlyteaching/effortful-retrieval/) whenever possible, rather than rushing to look up the specifics of techniques from earlier in the lesson.\n",
    "\n",
    "Your end goal is to profile an accurate `saxpy` kernel, without modifying `N`, to run in under *200,000 ns*. Check out [the solution](09-saxpy/solutions/02-saxpy-solution.cu) if you get stuck, and feel free to compile and profile it if you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c[0] = 0, c[1] = 0, c[2] = 0, c[3] = 0, c[4] = 0, \n",
      "c[4194299] = 0, c[4194300] = 0, c[4194301] = 0, c[4194302] = 0, c[4194303] = 0, \n"
     ]
    }
   ],
   "source": [
    "!nvcc -o saxpy 09-saxpy/01-saxpy.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c[0] = 0, c[1] = 0, c[2] = 0, c[3] = 0, c[4] = 0, \n",
      "c[4194299] = 0, c[4194300] = 0, c[4194301] = 0, c[4194302] = 0, c[4194303] = 0, \n",
      "Generating '/tmp/nsys-report-878d.qdstrm'\n",
      "[1/8] [========================100%] report51.nsys-rep\n",
      "[2/8] [========================100%] report51.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report51.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls  Avg (ns)   Med (ns)   Min (ns)  Max (ns)  StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ---------  ---------  --------  --------  -----------  ----------------------\n",
      "     64.3        166259676         21  7917127.4  7285645.0      2580  51082679   11353060.2  poll                  \n",
      "     17.5         45222109         15  3014807.3    26121.0       220  20559475    7063742.5  sem_timedwait         \n",
      "     16.1         41706010        503    82914.5    10690.0       380   8320673     474727.9  ioctl                 \n",
      "      1.0          2668069         23   116003.0     5650.0       950    873800     288611.7  mmap                  \n",
      "      0.4           906213         27    33563.4     4621.0      2970    551795     104591.6  mmap64                \n",
      "      0.2           453341         44    10303.2     9740.5      3700     28881       4387.4  open64                \n",
      "      0.2           399560          3   133186.7   100085.0     74924    224551      80117.7  sem_wait              \n",
      "      0.1           224280          5    44856.0    37572.0     29361     61233      15141.0  pthread_create        \n",
      "      0.1           162509         13    12500.7    14341.0       980     19141       4774.7  write                 \n",
      "      0.1           155187         29     5351.3     3540.0      1370     24431       5507.1  fopen                 \n",
      "      0.0            50712         26     1950.5       60.0        50     49032       9602.8  fgets                 \n",
      "      0.0            35802          6     5967.0     6911.0      2480      7810       2252.8  open                  \n",
      "      0.0            33481         52      643.9      475.0       160      6430        880.8  fcntl                 \n",
      "      0.0            28701          9     3189.0     3160.0      1790      5421       1145.4  munmap                \n",
      "      0.0            25603         22     1163.8      965.0       550      4210        764.5  fclose                \n",
      "      0.0            21181         16     1323.8     1085.0       420      3590        941.3  read                  \n",
      "      0.0            15751          2     7875.5     7875.5      3740     12011       5848.5  socket                \n",
      "      0.0            11220          1    11220.0    11220.0     11220     11220          0.0  connect               \n",
      "      0.0             8430          5     1686.0     1850.0       100      3110       1464.1  fread                 \n",
      "      0.0             6121         64       95.6       75.0        40       160         46.4  pthread_mutex_trylock \n",
      "      0.0             5301          1     5301.0     5301.0      5301      5301          0.0  pipe2                 \n",
      "      0.0             2100          1     2100.0     2100.0      2100      2100          0.0  bind                  \n",
      "      0.0              970          1      970.0      970.0       970       970          0.0  listen                \n",
      "      0.0              360          1      360.0      360.0       360       360          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)   Med (ns)  Min (ns)  Max (ns)   StdDev (ns)          Name        \n",
      " --------  ---------------  ---------  ----------  --------  --------  ---------  -----------  --------------------\n",
      "     88.2        107950645          3  35983548.3   23261.0     18841  107908543   62288872.6  cudaMallocManaged   \n",
      "     11.4         13890729          3   4630243.0  870120.0    845779   12174830    6533815.3  cudaFree            \n",
      "      0.4           468702          6     78117.0   30831.0      4190     213320      96249.8  cudaMemPrefetchAsync\n",
      "      0.0            39632          3     13210.7    7891.0      6520      25221      10423.8  cudaLaunchKernel    \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)             Name           \n",
      " --------  ---------------  ---------  --------  --------  --------  --------  -----------  --------------------------\n",
      "     53.2            67392          2   33696.0   33696.0     32096     35296       2262.7  initData(int *, int)      \n",
      "     46.8            59360          1   59360.0   59360.0     59360     59360          0.0  saxpy(int *, int *, int *)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "     99.7          3845552     24  160231.3  160239.0    160063    160544        121.8  [CUDA Unified Memory memcpy DtoH]\n",
      "      0.3            13247      2    6623.5    6623.5      6623      6624          0.7  [CUDA Unified Memory memcpy HtoD]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "     50.332     24     2.097     2.097     2.097     2.097        0.000  [CUDA Unified Memory memcpy DtoH]\n",
      "      0.131      2     0.066     0.066     0.066     0.066        0.000  [CUDA Unified Memory memcpy HtoD]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report51.nsys-rep\n",
      "    /dli/task/report51.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./saxpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11869153.0\n",
    "# 793630.0\n",
    "# 698398.0\n",
    "# 646238"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "```\n",
    "[5/8] Executing 'cuda_api_sum' stats report\n",
    "\n",
    " Time (%)  Total Time (ns)  Num Calls   Avg (ns)   Med (ns)   Min (ns)  Max (ns)   StdDev (ns)          Name         \n",
    " --------  ---------------  ---------  ----------  ---------  --------  ---------  -----------  ---------------------\n",
    "     94.2        123448777          3  41149592.3    50842.0     31742  123366193   71201665.4  cudaMallocManaged    \n",
    "      2.8          3648109          1   3648109.0  3648109.0   3648109    3648109          0.0  cudaDeviceSynchronize\n",
    "      1.7          2204529          3    734843.0   718605.0    699825     786099      45371.3  cudaFree             \n",
    "      1.4          1777857          3    592619.0   258683.0     14510    1504664     799233.8  cudaMemPrefetchAsync \n",
    "      0.0            36972          1     36972.0    36972.0     36972      36972          0.0  cudaLaunchKernel     \n",
    "\n",
    "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
    "\n",
    " Time (%)  Total Time (ns)  Instances  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)             Name           \n",
    " --------  ---------------  ---------  --------  --------  --------  --------  -----------  --------------------------\n",
    "    100.0           105664          1  105664.0  105664.0    105664    105664          0.0  saxpy(int *, int *, int *)\n",
    "\n",
    "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
    "\n",
    " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
    " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
    "     99.6          3814588     24  158941.2  158784.0    158719    159456        244.7  [CUDA Unified Memory memcpy HtoD]\n",
    "      0.4            14494      4    3623.5    3663.5      1407      5760       2467.3  [CUDA Unified Memory memcpy DtoH]\n",
    "\n",
    "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
    "\n",
    " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
    " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
    "     50.332     24     2.097     2.097     2.097     2.097        0.000  [CUDA Unified Memory memcpy HtoD]\n",
    "      0.131      4     0.033     0.033     0.004     0.061        0.033  [CUDA Unified Memory memcpy DtoH]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
